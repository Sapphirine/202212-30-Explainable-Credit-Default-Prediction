{"cells":[{"cell_type":"markdown","id":"3c7350cb","metadata":{},"source":["# Credit Default Prediction on Amex Dataset"]},{"cell_type":"markdown","id":"27117702","metadata":{},"source":["### Importing the necessary libraries"]},{"cell_type":"code","execution_count":1,"id":"7fe43cb9","metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","\n","import pyspark\n","from pyspark import StorageLevel\n","from pyspark.sql import (\n","    SparkSession, \n","    types, \n","    functions as F,\n",")\n","from pyspark.sql.functions import (\n","    col,\n","    isnan,\n","    when,\n","    count,\n",")\n","from pyspark.ml import Pipeline \n","from pyspark.ml.feature import (\n","    OneHotEncoder, \n","    StringIndexer, \n","    VectorAssembler, \n","    Imputer,\n",")\n","from pyspark.ml.classification import (\n","    LogisticRegression, \n","    LinearSVC,\n","    DecisionTreeClassifier,\n","    GBTClassifier,\n","    RandomForestClassifier,\n",")\n","from pyspark.ml.evaluation import (\n","    BinaryClassificationEvaluator,\n","    MulticlassClassificationEvaluator,\n",")\n","\n","import itertools\n","\n","import pickle"]},{"cell_type":"markdown","id":"b6523270","metadata":{},"source":["### Create a Spark Session"]},{"cell_type":"code","execution_count":2,"id":"cbce4d1a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/12/23 19:46:08 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/12/23 19:46:08 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/12/23 19:46:08 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/12/23 19:46:08 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["spark = SparkSession.builder \\\n","                    .appName(\"amex-app\") \\\n","                    .master(\"local[*]\") \\\n","                    .getOrCreate()"]},{"cell_type":"markdown","id":"f1c9dd7c","metadata":{},"source":["### Important Global Variables"]},{"cell_type":"code","execution_count":3,"id":"653bace1","metadata":{},"outputs":[],"source":["TRAIN_DATA_PATH = 'gs://icdp-bigdata-bucket/train_data.csv'\n","TRAIN_LABEL_PATH = 'gs://icdp-bigdata-bucket/train_labels.csv'"]},{"cell_type":"markdown","id":"2d4ead06","metadata":{},"source":["### Miscellaneous Utility Functions"]},{"cell_type":"code","execution_count":4,"id":"592136ca","metadata":{},"outputs":[],"source":["## Function to create a Schema Object for the Dataframe \n","def create_spark_schema(series):\n","    fields = list()\n","    \n","    for value in series: \n","        if value in string_dtypes:\n","            fields.append(\n","                types.StructField(\n","                    value, \n","                    types.StringType(), \n","                    True,\n","                )\n","            )\n","        elif value in date_dtypes:\n","            fields.append(\n","                types.StructField(\n","                    value, \n","                    types.DateType(), \n","                    True,\n","                )\n","            )\n","        elif value in integer_dtypes:\n","            fields.append(\n","                types.StructField(\n","                    value, \n","                    types.IntegerType(), \n","                    True,\n","                )\n","            )\n","        else:\n","            fields.append(\n","                types.StructField(\n","                    value, \n","                    types.FloatType(), \n","                    True,\n","                )\n","            )\n","    return types.StructType(fields)"]},{"cell_type":"code","execution_count":5,"id":"32d71d63","metadata":{},"outputs":[],"source":["#Add Suffix to List Elements\n","def add_suffix(names, suffix):\n","    return [name + suffix for name in names]"]},{"cell_type":"code","execution_count":6,"id":"fa6631e5","metadata":{},"outputs":[],"source":["# Drop Columns with Null values above a certain threshold\n","def dropNullColumns(df, threshold):\n","    \"\"\"\n","    This function drops columns containing all null values.\n","    :param df: A PySpark DataFrame\n","    \"\"\"\n","  \n","    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(\n","        c) for c in df.columns]).collect()[0].asDict()\n","    print(\"null counts calculated...\")\n","    df_count = df.count()\n","    col_to_drop = [k for k, v in null_counts.items() if v >(df_count * threshold)]  \n","    print(\"columns to drop found...\")\n","    df = df.drop(*col_to_drop)  \n","  \n","    return df, col_to_drop"]},{"cell_type":"markdown","id":"2ed7871e","metadata":{},"source":["### Reading the Dataframe"]},{"cell_type":"markdown","id":"5edb262b","metadata":{},"source":["#### Reading the First 20 rows only"]},{"cell_type":"code","execution_count":7,"id":"f6d129cd","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["train_df_temp = spark.read.option(\n","    \"header\", 'true',\n",").csv(\n","    TRAIN_DATA_PATH,\n",").limit(\n","    20\n",")\n","train_labels_temp = spark.read.option(\n","    \"header\", 'true',\n",").csv(\n","    TRAIN_LABEL_PATH,\n",").limit(\n","    20\n",")"]},{"cell_type":"markdown","id":"442847ac","metadata":{},"source":["#### Define Schema Using Sampled Temporary Dataframe"]},{"cell_type":"code","execution_count":8,"id":"d55f395e","metadata":{},"outputs":[],"source":["## Known Datatypes: \n","\n","string_dtypes = [\"customer_ID\", 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n","date_dtypes = ['S_2']\n","integer_dtypes = ['target']"]},{"cell_type":"code","execution_count":9,"id":"ada57716","metadata":{},"outputs":[],"source":["train_schema = create_spark_schema(train_df_temp.columns)\n","label_schema = create_spark_schema(train_labels_temp.columns)"]},{"cell_type":"markdown","id":"d7009e86","metadata":{},"source":["#### Remove Temp Datasets from Memory"]},{"cell_type":"code","execution_count":10,"id":"a6dc00e9","metadata":{},"outputs":[],"source":["train_df_temp.unpersist()\n","train_labels_temp.unpersist()\n","\n","del train_df_temp\n","del train_labels_temp"]},{"cell_type":"markdown","id":"f3078a13","metadata":{},"source":["#### Reading the Whole Dataset with the Inferred Schema"]},{"cell_type":"code","execution_count":11,"id":"ecb2836a","metadata":{},"outputs":[],"source":["train_df = spark.read.option(\n","    \"header\", \n","    \"true\",\n",").csv(\n","    TRAIN_DATA_PATH, \n","    schema=train_schema\n",")\n","label_df = spark.read.option(\n","    \"header\", \n","    \"true\",\n",").csv(\n","    TRAIN_LABEL_PATH, \n","    schema=label_schema,\n",")"]},{"cell_type":"code","execution_count":12,"id":"b8e0f1ee","metadata":{},"outputs":[],"source":["## Other categorization of the known dtypes\n","info_cols = ['customer_ID', 'S_2']\n","target_cols = ['target']\n","cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n","\n","\n","# Define Numeric Columns\n","excluded = info_cols + cat_cols\n","num_cols = [col for col in train_df.columns if col not in excluded]"]},{"cell_type":"markdown","id":"44ec3f59","metadata":{},"source":["### Preprocessing of the Dataset"]},{"cell_type":"markdown","id":"a10734c3","metadata":{},"source":["#### Dropping Null Columns"]},{"cell_type":"code","execution_count":13,"id":"7937c71c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/12/23 19:46:23 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["null counts calculated...\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 5:======================================================>(122 + 1) / 123]\r"]},{"name":"stdout","output_type":"stream","text":["columns to drop found...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["## Remove All Columns with More than 5% Missing Values\n","train_df, cols_to_drop = dropNullColumns(train_df, 0.05)"]},{"cell_type":"markdown","id":"3d1e5e15","metadata":{},"source":["#### Remove Less Important Column S_2"]},{"cell_type":"code","execution_count":14,"id":"43ebbf50","metadata":{},"outputs":[],"source":["## Remove the S_2 variable as the testing data and the training data are in different time periods \n","train_df = train_df.drop(\"S_2\")"]},{"cell_type":"code","execution_count":15,"id":"1743aec2","metadata":{},"outputs":[],"source":["cols_to_drop.append(\"S_2\")"]},{"cell_type":"markdown","id":"5fa1ba60","metadata":{},"source":["#### Converting Categorical Columns to Numeric using StringIndexer"]},{"cell_type":"code","execution_count":16,"id":"08f97637","metadata":{},"outputs":[],"source":["cat_columns_to_index = list(set(train_df.columns) & set(cat_cols))"]},{"cell_type":"code","execution_count":17,"id":"012564fb","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["cat_cols_indexed = add_suffix(cat_columns_to_index, \"_index\")\n","\n","## Create StringIndexer Object\n","indexer = StringIndexer(\n","    inputCols=cat_columns_to_index,\n","    outputCols=cat_cols_indexed,\n",")\n","indexer.setHandleInvalid(\"keep\")\n","indexer_model = indexer.fit(train_df)\n","\n","train_df = indexer_model.transform(train_df)"]},{"cell_type":"markdown","id":"557aa1fc","metadata":{},"source":["#### Impute values for numerical columns"]},{"cell_type":"code","execution_count":18,"id":"92d02af4","metadata":{},"outputs":[],"source":["num_columns_to_impute = list(set(train_df.columns) & set(num_cols))"]},{"cell_type":"code","execution_count":19,"id":"9d7257d6","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["num_cols_imputed = add_suffix(num_columns_to_impute, \"_imputed\")\n","\n","##Create Imputer\n","imputer = Imputer(\n","    inputCols=num_columns_to_impute,\n","    outputCols=num_cols_imputed,\n",")\n","imputer.setStrategy(\"median\")\n","\n","imputer_model = imputer.fit(train_df)\n","\n","train_df = imputer_model.transform(train_df)"]},{"cell_type":"markdown","id":"d88b3ea3","metadata":{},"source":["#### OneHotEncode the Categorical Columns"]},{"cell_type":"code","execution_count":20,"id":"fbe092b8","metadata":{},"outputs":[],"source":["cat_cols_ohe = add_suffix(cat_cols_indexed, \"_ohe\")\n","\n","### Create Ohe Object\n","ohe = OneHotEncoder(\n","    inputCols = cat_cols_indexed,\n","    outputCols = cat_cols_ohe,\n",")\n","\n","ohe_model = ohe.fit(train_df)\n","\n","train_df = ohe_model.transform(train_df)"]},{"cell_type":"code","execution_count":21,"id":"b092001f","metadata":{},"outputs":[],"source":["useful_cols = [\"customer_ID\"] + cat_cols_ohe + num_cols_imputed"]},{"cell_type":"markdown","id":"05ed168d","metadata":{},"source":["### Remove Unnecessary Columns and Aggregate"]},{"cell_type":"code","execution_count":22,"id":"2760ad5a","metadata":{},"outputs":[],"source":["train_df = train_df.select(*useful_cols)"]},{"cell_type":"code","execution_count":23,"id":"cdce9316","metadata":{},"outputs":[],"source":["new_num_cols = []\n","for num_col in num_cols_imputed:\n","    new_name = num_col.split(\"_\")[0] + \"_\" + num_col.split(\"_\")[1]\n","    new_num_cols.append(new_name)\n","    train_df = train_df.withColumnRenamed(num_col, new_name)\n","new_cat_cols = []\n","for cat_col in cat_cols_ohe:\n","    new_name = cat_col.split(\"_\")[0] + \"_\" + cat_col.split(\"_\")[1]\n","    new_cat_cols.append(new_name)\n","    train_df = train_df.withColumnRenamed(cat_col, new_name)"]},{"cell_type":"code","execution_count":24,"id":"39201add","metadata":{},"outputs":[],"source":["## Aggregation Functions\n","num_funcs = [\n","    (F.mean, \"_mean\"),\n","     (F.min, \"_min\"),\n","     (F.max, \"_max\"),\n","]\n","\n","cat_funcs = [\n","    (F.count, \"_count\"),\n","    (F.last, \"_last\"),\n","    (F.countDistinct, \"_nunique\"),\n","]"]},{"cell_type":"code","execution_count":25,"id":"76d18a11","metadata":{},"outputs":[],"source":["agg_num_args = [\n","    func(col).alias(col + suffix) \n","    for col, (func, suffix) in itertools.product(new_num_cols, num_funcs)]\n","\n","agg_cols_args = [\n","    func(col).alias(col + suffix) \n","    for col, (func, suffix) in itertools.product(new_cat_cols, cat_funcs)]\n","\n","# Combine numeric and categoric agg arguments\n","agg_args = agg_num_args + agg_cols_args"]},{"cell_type":"code","execution_count":26,"id":"547a8887","metadata":{},"outputs":[],"source":["train_df = train_df.groupBy(\"customer_ID\").agg(*agg_args)"]},{"cell_type":"code","execution_count":27,"id":"e7e33c44","metadata":{},"outputs":[],"source":["train_df = train_df.join(\n","    F.broadcast(label_df), \n","    on=\"customer_ID\",\n",")"]},{"cell_type":"code","execution_count":28,"id":"2ba16c32","metadata":{},"outputs":[],"source":["va_model = VectorAssembler(\n","    inputCols=train_df.drop(\n","        \"customer_ID\",\n","        \"target\",\n","    ).columns,\n","    outputCol=\"features\",\n","    handleInvalid=\"skip\",\n",")"]},{"cell_type":"code","execution_count":29,"id":"ad608946","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["train_df = va_model.transform(\n","    train_df,\n",").select(\n","    [\n","        \"customer_ID\", \n","        \"features\", \n","        \"target\",\n","    ]\n",").persist(\n","    StorageLevel.DISK_ONLY,\n",")"]},{"cell_type":"markdown","id":"cb3763d7","metadata":{},"source":["### Train Test Split"]},{"cell_type":"code","execution_count":30,"id":"e1885f91","metadata":{},"outputs":[],"source":["train_split, test_split = train_df.randomSplit(weights = [0.8, 0.2], seed = 42)"]},{"cell_type":"markdown","id":"3a100618","metadata":{},"source":["### Fit Models"]},{"cell_type":"markdown","id":"da5cdeef","metadata":{},"source":["#### Logistic Regression"]},{"cell_type":"code","execution_count":null,"id":"2977e2f5","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["22/12/23 19:04:28 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n","22/12/23 19:04:28 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n","                                                                                \r"]}],"source":["lr = LogisticRegression(\n","    featuresCol=\"features\",\n","    labelCol=\"target\",\n",")\n","lr_model = lr.fit(train_split)"]},{"cell_type":"code","execution_count":null,"id":"b57343e9","metadata":{},"outputs":[],"source":["lr_preds = lr_model.transform(test_split)"]},{"cell_type":"code","execution_count":null,"id":"8363b257","metadata":{},"outputs":[],"source":["binEval = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"target\",metricName=\"areaUnderROC\")\n","multiEval = MulticlassClassificationEvaluator(labelCol = \"target\", predictionCol = \"prediction\")"]},{"cell_type":"code","execution_count":null,"id":"ccb244a1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["AUCROC:  0.8486418824365403\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Accuracy:  0.8894452036075431\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["F1 Score:  0.8885082375446438\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Weighted Precision:  0.8879481704342884\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 569:===================================================> (195 + 4) / 200]\r"]},{"name":"stdout","output_type":"stream","text":["Weighted Recall:  0.8894452036075431\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["print(\"AUCROC: \", binEval.evaluate(lr_preds))\n","print(\"Accuracy: \", multiEval.evaluate(lr_preds, {multiEval.metricName: \"accuracy\"}))\n","print(\"F1 Score: \", multiEval.evaluate(lr_preds, {multiEval.metricName: \"f1\"}))\n","print(\"Weighted Precision: \", multiEval.evaluate(lr_preds, {multiEval.metricName: \"weightedPrecision\"}))\n","print(\"Weighted Recall: \", multiEval.evaluate(lr_preds, {multiEval.metricName: \"weightedRecall\"}))"]},{"cell_type":"markdown","id":"bf93abbc","metadata":{},"source":["#### Decision Tree Classifier"]},{"cell_type":"code","execution_count":null,"id":"62d44ca1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["dt = DecisionTreeClassifier(\n","    featuresCol=\"features\", \n","    labelCol=\"target\"\n",")\n","dt_model = dt.fit(train_split)"]},{"cell_type":"code","execution_count":null,"id":"52d108df","metadata":{},"outputs":[],"source":["dt_preds = dt_model.transform(test_split)"]},{"cell_type":"code","execution_count":null,"id":"d729d6b2","metadata":{},"outputs":[],"source":["binEval = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"target\",metricName=\"areaUnderROC\")\n","multiEval = MulticlassClassificationEvaluator(labelCol = \"target\", predictionCol = \"prediction\")"]},{"cell_type":"code","execution_count":null,"id":"0ecdd9da","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["AUCROC:  0.8200578293739653\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Accuracy:  0.8605848592511616\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["F1 Score:  0.8606816617623854\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Weighted Precision:  0.860781015714613\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 641:==================================================>  (190 + 4) / 200]\r"]},{"name":"stdout","output_type":"stream","text":["Weighted Recall:  0.8605848592511615\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["print(\"AUCROC: \", binEval.evaluate(dt_preds))\n","print(\"Accuracy: \", multiEval.evaluate(dt_preds, {multiEval.metricName: \"accuracy\"}))\n","print(\"F1 Score: \", multiEval.evaluate(dt_preds, {multiEval.metricName: \"f1\"}))\n","print(\"Weighted Precision: \", multiEval.evaluate(dt_preds, {multiEval.metricName: \"weightedPrecision\"}))\n","print(\"Weighted Recall: \", multiEval.evaluate(dt_preds, {multiEval.metricName: \"weightedRecall\"}))"]},{"cell_type":"markdown","id":"54223792","metadata":{},"source":["#### Linear SVC"]},{"cell_type":"code","execution_count":null,"id":"4314502b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["svc = LinearSVC(\n","    featuresCol=\"features\",\n","    labelCol=\"target\",\n",")\n","svc_model = svc.fit(train_split)"]},{"cell_type":"code","execution_count":null,"id":"220b9f1a","metadata":{},"outputs":[],"source":["svc_preds = svc_model.transform(test_split)"]},{"cell_type":"code","execution_count":null,"id":"280a3467","metadata":{},"outputs":[],"source":["binEval = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"target\",metricName=\"areaUnderROC\")\n","multiEval = MulticlassClassificationEvaluator(labelCol = \"target\", predictionCol = \"prediction\")"]},{"cell_type":"code","execution_count":null,"id":"215693a3","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["AUCROC:  0.8491408126883391\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Accuracy:  0.8890625854058486\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["F1 Score:  0.8882420515505906\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Weighted Precision:  0.8877104482580733\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 1616:==================================================> (196 + 4) / 200]\r"]},{"name":"stdout","output_type":"stream","text":["Weighted Recall:  0.8890625854058486\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["print(\"AUCROC: \", binEval.evaluate(svc_preds))\n","print(\"Accuracy: \", multiEval.evaluate(svc_preds, {multiEval.metricName: \"accuracy\"}))\n","print(\"F1 Score: \", multiEval.evaluate(svc_preds, {multiEval.metricName: \"f1\"}))\n","print(\"Weighted Precision: \", multiEval.evaluate(svc_preds, {multiEval.metricName: \"weightedPrecision\"}))\n","print(\"Weighted Recall: \", multiEval.evaluate(svc_preds, {multiEval.metricName: \"weightedRecall\"}))"]},{"cell_type":"markdown","id":"56e8c5e5","metadata":{},"source":["#### Gradient Boosted Trees Classifier"]},{"cell_type":"code","execution_count":null,"id":"122d53a8","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/12/23 21:07:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1004.2 KiB\n","22/12/23 21:07:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1004.7 KiB\n","22/12/23 21:07:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1005.3 KiB\n","22/12/23 21:07:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1006.4 KiB\n","22/12/23 21:07:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1008.7 KiB\n","22/12/23 21:08:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1011.5 KiB\n","22/12/23 21:08:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1012.0 KiB\n","22/12/23 21:08:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1012.6 KiB\n","22/12/23 21:08:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1013.7 KiB\n","22/12/23 21:08:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1016.0 KiB\n","22/12/23 21:08:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1018.7 KiB\n","22/12/23 21:08:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1019.1 KiB\n","22/12/23 21:08:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1019.7 KiB\n","22/12/23 21:08:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1020.8 KiB\n","22/12/23 21:08:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1023.1 KiB\n","22/12/23 21:08:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1025.8 KiB\n","22/12/23 21:08:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1026.2 KiB\n","22/12/23 21:08:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1026.8 KiB\n","22/12/23 21:09:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1027.9 KiB\n","22/12/23 21:09:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1030.2 KiB\n","22/12/23 21:09:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1032.9 KiB\n","22/12/23 21:09:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1033.3 KiB\n","22/12/23 21:09:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1033.9 KiB\n","22/12/23 21:09:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1035.1 KiB\n","22/12/23 21:09:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1037.3 KiB\n","22/12/23 21:09:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1040.0 KiB\n","22/12/23 21:09:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1040.5 KiB\n","22/12/23 21:09:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1041.0 KiB\n","22/12/23 21:09:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1042.2 KiB\n","22/12/23 21:09:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1044.4 KiB\n","22/12/23 21:10:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1047.1 KiB\n","22/12/23 21:10:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1047.6 KiB\n","22/12/23 21:10:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1048.1 KiB\n","22/12/23 21:10:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1049.3 KiB\n","22/12/23 21:10:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1051.5 KiB\n","22/12/23 21:10:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1054.2 KiB\n","22/12/23 21:10:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1054.7 KiB\n","22/12/23 21:10:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1055.2 KiB\n","22/12/23 21:10:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1056.4 KiB\n","22/12/23 21:10:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1058.6 KiB\n","22/12/23 21:10:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1061.3 KiB\n","22/12/23 21:10:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1061.8 KiB\n","22/12/23 21:11:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1062.3 KiB\n","22/12/23 21:11:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1063.5 KiB\n","22/12/23 21:11:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1065.8 KiB\n","22/12/23 21:11:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1068.4 KiB\n","22/12/23 21:11:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1068.9 KiB\n","22/12/23 21:11:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1069.5 KiB\n","22/12/23 21:11:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1070.6 KiB\n","22/12/23 21:11:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1072.9 KiB\n","22/12/23 21:11:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n","22/12/23 21:11:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1076.0 KiB\n","22/12/23 21:11:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1076.6 KiB\n","22/12/23 21:11:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1077.7 KiB\n","22/12/23 21:12:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1080.0 KiB\n","22/12/23 21:12:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1082.6 KiB\n","22/12/23 21:12:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1083.1 KiB\n","22/12/23 21:12:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1083.7 KiB\n","22/12/23 21:12:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1084.8 KiB\n","22/12/23 21:12:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1087.1 KiB\n","22/12/23 21:12:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1089.7 KiB\n","22/12/23 21:12:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1090.2 KiB\n","22/12/23 21:12:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1090.8 KiB\n","22/12/23 21:12:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1091.9 KiB\n","22/12/23 21:12:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1094.2 KiB\n","22/12/23 21:12:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1096.8 KiB\n","22/12/23 21:13:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1097.3 KiB\n","22/12/23 21:13:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1097.9 KiB\n","22/12/23 21:13:12 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1099.0 KiB\n","22/12/23 21:13:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1101.3 KiB\n","22/12/23 21:13:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1103.9 KiB\n","22/12/23 21:13:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1104.4 KiB\n","22/12/23 21:13:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1105.0 KiB\n","22/12/23 21:13:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1106.1 KiB\n","22/12/23 21:13:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1108.4 KiB\n","22/12/23 21:13:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1111.1 KiB\n","22/12/23 21:13:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1111.5 KiB\n","22/12/23 21:13:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1112.1 KiB\n","22/12/23 21:14:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1113.2 KiB\n","22/12/23 21:14:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1115.5 KiB\n","22/12/23 21:14:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1118.2 KiB\n","22/12/23 21:14:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1118.6 KiB\n","22/12/23 21:14:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1119.2 KiB\n","22/12/23 21:14:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1120.3 KiB\n","22/12/23 21:14:33 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1122.6 KiB\n","22/12/23 21:14:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1125.3 KiB\n","22/12/23 21:14:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1125.7 KiB\n","22/12/23 21:14:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1126.3 KiB\n","22/12/23 21:14:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1127.5 KiB\n","22/12/23 21:14:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1129.7 KiB\n","22/12/23 21:15:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1132.4 KiB\n","22/12/23 21:15:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1132.9 KiB\n","22/12/23 21:15:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1133.4 KiB\n","22/12/23 21:15:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1134.6 KiB\n","22/12/23 21:15:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1136.8 KiB\n","                                                                                \r"]}],"source":["gbt = GBTClassifier(\n","    featuresCol = 'features', \n","    labelCol = 'target',\n",")\n","gbt_model = gbt.fit(train_split)"]},{"cell_type":"code","execution_count":null,"id":"cc0a86ff","metadata":{},"outputs":[],"source":["gbt_preds = gbt_model.transform(test_split)"]},{"cell_type":"code","execution_count":null,"id":"9bc73093","metadata":{},"outputs":[],"source":["binEval = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"target\",metricName=\"areaUnderROC\")\n","multiEval = MulticlassClassificationEvaluator(labelCol = \"target\", predictionCol = \"prediction\")"]},{"cell_type":"code","execution_count":null,"id":"23d13f69","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/12/23 21:15:31 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1029.8 KiB\n","22/12/23 21:15:31 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n","22/12/23 21:15:31 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["AUCROC:  0.8426669815866115\n"]},{"name":"stderr","output_type":"stream","text":["22/12/23 21:15:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1037.4 KiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Accuracy:  0.8781962284777262\n"]},{"name":"stderr","output_type":"stream","text":["22/12/23 21:15:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1037.4 KiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["F1 Score:  0.8782554150262379\n"]},{"name":"stderr","output_type":"stream","text":["22/12/23 21:16:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1037.4 KiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Weighted Precision:  0.8783158938891666\n"]},{"name":"stderr","output_type":"stream","text":["22/12/23 21:16:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1037.4 KiB\n","[Stage 463:===================================================> (195 + 4) / 200]\r"]},{"name":"stdout","output_type":"stream","text":["Weighted Recall:  0.878196228477726\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["print(\"AUCROC: \", binEval.evaluate(gbt_preds))\n","print(\"Accuracy: \", multiEval.evaluate(gbt_preds, {multiEval.metricName: \"accuracy\"}))\n","print(\"F1 Score: \", multiEval.evaluate(gbt_preds, {multiEval.metricName: \"f1\"}))\n","print(\"Weighted Precision: \", multiEval.evaluate(gbt_preds, {multiEval.metricName: \"weightedPrecision\"}))\n","print(\"Weighted Recall: \", multiEval.evaluate(gbt_preds, {multiEval.metricName: \"weightedRecall\"}))"]},{"cell_type":"markdown","id":"f176c4f2","metadata":{},"source":["### Save Models and Meta Data"]},{"cell_type":"markdown","id":"753e36b7","metadata":{},"source":["#### Data to Save"]},{"cell_type":"code","execution_count":null,"id":"542f6062","metadata":{},"outputs":[],"source":["meta_data = {\n","    #\"spark_session\": spark,\n","    \"schema\":{\n","        \"train_schema\": train_schema,\n","        \"label_schema\": label_schema,\n","    },\n","    \"column_names\":{\n","        \"cols_to_drop\": cols_to_drop,\n","        \"cat_columns_to_index\": cat_columns_to_index,\n","        \"num_cols_imputed\": num_cols_imputed,\n","        \"cat_cols_ohe\": cat_cols_ohe,\n","        \"useful_cols\": useful_cols,\n","    },\n","}"]},{"cell_type":"code","execution_count":null,"id":"f655d0b3","metadata":{},"outputs":[],"source":["with open('/home/aap2239/interpretable-credit-default-prediction/meta_data.pkl', 'wb') as handle:\n","    pickle.dump(meta_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"]},{"cell_type":"code","execution_count":null,"id":"714c22ec","metadata":{},"outputs":[],"source":["from google.cloud import storage\n","\n","PROJECT = 'big-data-86948'\n","BUCKET_NAME = 'icdp-bigdata-bucket'\n","first_layer = \"icdp_deployment/\"\n","second_layer_meta = \"meta_data/\"\n","second_layer_objects = \"objects/\"\n","storage_client = storage.Client(project=PROJECT)"]},{"cell_type":"code","execution_count":null,"id":"72e51011","metadata":{},"outputs":[],"source":["def create_folder(bucket_name, folder_name):\n","    bucket = storage_client.get_bucket(bucket_name)\n","    blob = bucket.blob(folder_name)\n","    blob.upload_from_string('', content_type='application/x-www-form-urlencoded;charset=UTF-8')"]},{"cell_type":"code","execution_count":null,"id":"63c7b77c","metadata":{},"outputs":[],"source":["def upload_blob(bucket_name, source_file_name, destination_blob_name):\n","    \"\"\"Uploads a file to the bucket. https://cloud.google.com/storage/docs/ \"\"\"\n","    bucket = storage_client.get_bucket(bucket_name)\n","    blob = bucket.blob(destination_blob_name)\n","    blob.upload_from_filename(source_file_name)\n","    print('File {} uploaded to {}.'.format(\n","        source_file_name,\n","        destination_blob_name))"]},{"cell_type":"code","execution_count":null,"id":"5dc335cb","metadata":{},"outputs":[],"source":["create_folder(BUCKET_NAME, first_layer)"]},{"cell_type":"code","execution_count":null,"id":"5e99cca9","metadata":{},"outputs":[],"source":["create_folder(BUCKET_NAME, \"icdp_deployment/\"+second_layer_meta)\n","create_folder(BUCKET_NAME, \"icdp_deployment/\"+second_layer_objects)"]},{"cell_type":"code","execution_count":null,"id":"17e3ad9d","metadata":{},"outputs":[],"source":["upload_blob(BUCKET_NAME, \"/home/aap2239/interpretable-credit-default-prediction/meta_data.pkl\", \"icdp_deployment/\"+second_layer_meta+\"meta_data.pkl\")"]},{"cell_type":"code","execution_count":null,"id":"408faba2","metadata":{},"outputs":[],"source":["!rm /home/aap2239/interpretable-credit-default-prediction/meta_data.pkl"]},{"cell_type":"markdown","id":"02998aea","metadata":{},"source":["#### Models to Save "]},{"cell_type":"code","execution_count":null,"id":"688116e2","metadata":{},"outputs":[],"source":["indexer_model.save(\"gs://icdp-bigdata-bucket/icdp_deployment/objects/indexer_model\")"]},{"cell_type":"code","execution_count":null,"id":"5ee59c0c","metadata":{},"outputs":[],"source":["imputer_model.save(\"gs://icdp-bigdata-bucket/icdp_deployment/objects/imputer_model\")"]},{"cell_type":"code","execution_count":null,"id":"cd43b1b2","metadata":{},"outputs":[],"source":["ohe_model.save(\"gs://icdp-bigdata-bucket/icdp_deployment/objects/ohe_model\")"]},{"cell_type":"code","execution_count":null,"id":"fd1679f6","metadata":{},"outputs":[],"source":["va_model.save(\"gs://icdp-bigdata-bucket/icdp_deployment/objects/va_model\")"]},{"cell_type":"code","execution_count":null,"id":"8955dc98","metadata":{},"outputs":[],"source":["lr_model.save(\"gs://icdp-bigdata-bucket/icdp_deployment/objects/lr__model\")"]},{"cell_type":"code","execution_count":null,"id":"a57ca17b","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}